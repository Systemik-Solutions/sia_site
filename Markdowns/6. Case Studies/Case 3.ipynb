{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequencies of \"has\" and \"hath\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates using the <ins>SIA API</ins> to analyse the usage of words \"has\" and \"hath\" in Shakespear plays over the years.\n",
    "\n",
    "_@Hugh, some explaination about the purpose of doing this experiment_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, a number of dependency modules are imported here.\n",
    "\n",
    "- `requests`: is used for sending requests to the SIA API.\n",
    "- `matplotlib.pyplot`: we use the Matplotlib to create the chart based on the results from the SIA API.\n",
    "- `matplotlib.ticker`: is used for setting up the plot ticks.\n",
    "- `IPython.display`: is used for rendering the results from the SIA API as HTML tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FixedLocator\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the API endpoint here. In this example, we use the <ins>Word Frequencies API</ins>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_url = \"https://sia.ardc-hdcl-sia-iaw.cloud.edu.au/api/v1/word-frequencies\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the API, an API key is requried to authenticate the requests. The API key must be specified in a custom HTTP header `X-API-KEY` and sent along with every request.\n",
    "\n",
    "You should use your own API keys for your own notebooks and always keep your keys confidential. Read more about <ins>how to create API keys</ins> in SIA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_key = \"255446bcdde7ca9fe776258d09e8411bbb8d1cade2ebd6aba440f80f6817c3fd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we start to prepare the request data which we can send to the Word Frequencies API. In this example, we are going to use a text set containing 20 Shakespear plays which has been already uploaded in the SIA platform. Instead of passing the actual text contents to the API, we can tell the API to use one of the texts or text sets from SIA by specifying its ID.\n",
    "\n",
    "The URL of a text/text set page from <ins>SIA Application</ins> indicates the ID of that text/text set. For example:\n",
    "\n",
    "```\n",
    "https://sia.ardc-hdcl-sia-iaw.cloud.edu.au/text-sets/86\n",
    "```\n",
    "\n",
    "In this case, the ID of the \"20 Shakespear plays\" text set is `86`.\n",
    "\n",
    "We will also pass serveral word frequecies options to the Word Frequencies API. These options are:\n",
    "\n",
    "- `blockMethod`: We set the block method to `0`(By text), which makes each text from the text set as a single segment.\n",
    "- `showMetadata`: These Shakespear plays from SIA contain associated metadata. We are going to use the `Date of first performance` metadata to dertermin the year of a play. Therefore, we set this option to `true` to ask the API to return the metadata of texts in addition.\n",
    "- `outputSpecialWords`: As we are only interested in the words \"has\" and \"hath\", we tell the API to only return the frequencies of these two words.\n",
    "- `outputSpecialWordsOption`: We want to calculate the frequencies for all words but only ouput the frequencies of the words \"has\" and \"hath\".\n",
    "- `excludeWords`: We are excluding some common punctuation marks from our analysis.\n",
    "\n",
    "To view more details about options of Word Frequencies API, read the <ins>API documentation</ins>.\n",
    "\n",
    "_@Hugh: more explainations about these options may be specified here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data = {\n",
    "    'textSet': 86,\n",
    "    'option': {\n",
    "        'blockMethod' : 0,         #Segment by text\n",
    "        'showMetadata' : True,\n",
    "        'outputSpecialWords': [\"has\",\"hath\"],\n",
    "        'outputSpecialWordsOption': 0,  # Only show the word frequencies of special words\n",
    "        'excludeWords': [\"[\",\"\\\\\", \"]\", \"_\", \"`\", \"!\", \"\\\"\", \"#\", \"%\", \"'\", \"(\", \")\", \"+\", \",\", \"-\", \"â€“\", \".\", \"/\", \":\", \";\", \"{\", \"|\", \"}\", \"=\", \"~\", \"?\" ],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SIA API accept JSON as the request data. Here we have constructed a Python dictionary object with the text set identifier and the word frequencies options. Next we are going to put all things together and use the [Requests](https://requests.readthedocs.io/en/latest/) module to send the request to the SIA API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make API request\n",
    "response = requests.post(request_url, json=request_data, headers={\"X-API-KEY\": api_key}, timeout=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have specified the API endpoint, request data we defined earlier and the `X-API-KEY` HTTP header for the API request and received the response. Please note that the API call can take serveral minutes to finish based on the size of the text or text set. Therefore, we have set the request timeout to `1200` seconds.\n",
    "\n",
    "Before we start unpacking the response data, we want to make sure the API call was successful by checking the HTTP response code. Read the <ins>API documentation</ins> for all error codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{response.status_code} {response.reason}\")\n",
    "assert response.status_code == 200\n",
    "response_data = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the response data ready from Word Frequencies API. We firstly want to have a glance of the proportions of words \"has\" and \"hath\" used in each year from a table. We are going to create the tabular data from response data. Read more about the <ins>response data</ins> of Word Frequencies API.\n",
    "\n",
    "We will have two header rows for the table. The first header row will display the play names, and the second will list the years of the plays. Instead using the word frequency numbers, we will calculate the proportions of words and display them in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_headers = []\n",
    "table_rows = []\n",
    "for block in response_data['blocks']:\n",
    "    # Add block names as the first header row.\n",
    "    if len(table_headers) == 0:\n",
    "        table_headers.append(['Word'])\n",
    "    table_headers[0].append(block['name'])\n",
    "\n",
    "    # Add the years as the second header row.\n",
    "    if len(table_headers) == 1:\n",
    "        table_headers.append(['Year'])\n",
    "    table_headers[1].append(block['metadata']['Date of first performance'])\n",
    "\n",
    "    # Add data rows.\n",
    "    for i in range(len(block['frequencies'])):\n",
    "        frequency = block['frequencies'][i]\n",
    "        # Check whether the row has been created. If not, initialise the row with the word text.\n",
    "        if i > len(table_rows) - 1:\n",
    "            table_rows.append([frequency['word']])\n",
    "        # Calculate the fequency proportion.\n",
    "        proportion = str(round(frequency['value'] / block['size'] * 100, 4)) + '%'\n",
    "        # Append the word frequency proportion to its corresponding row.\n",
    "        table_rows[i].append(proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to generate the HTML markups based on the tabular data and render it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the opening tags of container and table elements.\n",
    "html = '<div style=\"overflow-x: auto; margin-top: 40px;\"><table border=\"1\">'\n",
    "\n",
    "# Append table headers.\n",
    "for table_row in table_headers:\n",
    "    html += '<tr>'\n",
    "    for table_cell in table_row:\n",
    "        html += f'<th style=\"white-space: nowrap;\">{table_cell}</td>'\n",
    "    html += '</tr>'\n",
    "\n",
    "# Append HTML for table rows.\n",
    "for table_row in table_rows:\n",
    "    html += '<tr>'\n",
    "    for table_cell in table_row:\n",
    "        html += f'<td>{table_cell}</td>'\n",
    "    html += '</tr>'\n",
    "\n",
    "# Close the table and container elements.\n",
    "html += '</table></div>'\n",
    "\n",
    "# Render the HTML.\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_@Hugh: more insights about the table can be described here_\n",
    "\n",
    "Next, we will plot the usage of words \"has\" and \"hath\" over the years in a line chart. We will firstly aggregate the word frequencies in each year. Each block from the response data will have the `metadata` about the play. We will use the metadata `Date of first performance` to get the years of play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_to_has_count = {}\n",
    "year_to_hath_count = {}\n",
    "year_to_word_count = {}\n",
    "for block in response_data['blocks']:\n",
    "    # Get the year from metadata.\n",
    "    year = int(block['metadata']['Date of first performance'])\n",
    "    for i in range(len(block['frequencies'])):\n",
    "        frequency = block['frequencies'][i]\n",
    "        if frequency['word'] == 'has':\n",
    "            # Build up the count of word 'has' of that year.\n",
    "            year_to_has_count[year] = year_to_has_count.get(year, 0) + frequency['value']\n",
    "        elif frequency['word'] == 'hath':\n",
    "            # Build up the count of word 'hath' of that year.\n",
    "            year_to_hath_count[year] = year_to_hath_count.get(year, 0) + frequency['value']\n",
    "        # Build up the total word count of that year.\n",
    "        year_to_word_count[year] = year_to_word_count.get(year, 0) + block['size']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created three dictionaries:\n",
    "\n",
    "- `year_to_has_count`: contains the frequency of word \"has\" from each year.\n",
    "- `year_to_hath_count`: contains the frequency of word \"hath\" from each year.\n",
    "- `year_to_word_count`: contains the total number of words from each year.\n",
    "\n",
    "We then need to sort the years and calculate the proportions of words in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(year_to_word_count.keys())\n",
    "has_proportions = [(year_to_has_count[key] / year_to_word_count[key]) * 100 for key in sorted(year_to_has_count.keys())]\n",
    "hath_proportions = [(year_to_hath_count[key] / year_to_word_count[key]) * 100 for key in sorted(year_to_hath_count.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created three lists which we can use for the chart:\n",
    "\n",
    "- `years`: the list of years sorted from the earliest to the latest.\n",
    "- `has_proportions`: the proportions of word \"has\" in each year sorted by year.\n",
    "- `hath_proportions`: the proportions of word \"hath\" in each year sorted by year.\n",
    "\n",
    "Now we will use the [Matplotlib](https://matplotlib.org/) library to create a line chart from these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(years, has_proportions, label='HAS', marker='o')\n",
    "plt.plot(years, hath_proportions, label='HATH', marker='o')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Average percentages of HAS and HATH in Shakespeare plays, by year of first performance')\n",
    "\n",
    "plt.xlim(1590, 1615)\n",
    "y_ticks = plt.gca().get_yticks()\n",
    "plt.gca().yaxis.set_major_locator(FixedLocator(y_ticks))\n",
    "plt.gca().set_yticklabels(['{:.2f}%'.format(y) for y in y_ticks])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_@Hugh: some insights about the chart_"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
