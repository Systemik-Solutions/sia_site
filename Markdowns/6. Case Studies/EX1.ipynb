{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 most frequent 6-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use the <ins>SIA API</ins> to find the top 10 most frequent 6-grams from a set of Sharkspeare plays.\n",
    "\n",
    "_@Hugh, some explaination about the purpose of doing this experiment_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, a number of dependency modules are imported here.\n",
    "\n",
    "- `requests`: is used for sending requests to the SIA API.\n",
    "- `matplotlib.pyplot`: we use the Matplotlib to create the chart based on the results from the SIA API.\n",
    "- `IPython.display`: is used for rendering the results from the SIA API as HTML tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the API endpoint here. In this example, we use the <ins>Word Frequencies API</ins>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_url = \"https://sia.ardc-hdcl-sia-iaw.cloud.edu.au/api/v1/word-frequencies\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to use the API, an API key is requried to authenticate the requests. The API key must be specified in a custom HTTP header `X-API-KEY` and sent along with every request.\n",
    "\n",
    "You should use your own API keys for your own notebooks and always keep your keys confidential. Read more about <ins>how to create API keys</ins> in SIA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_key = \"255446bcdde7ca9fe776258d09e8411bbb8d1cade2ebd6aba440f80f6817c3fd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we start to prepare the request data which we can send to the Word Frequencies API. In this example, we are going to use a text set containing 20 Shakespear plays which has been already uploaded in the SIA platform. Instead of passing the actual text contents to the API, we can tell the API to use one of the texts or text sets from SIA by specifying its ID.\n",
    "\n",
    "The URL of a text/text set page from <ins>SIA Application</ins> indicates the ID of that text/text set. For example:\n",
    "\n",
    "```\n",
    "https://sia.ardc-hdcl-sia-iaw.cloud.edu.au/text-sets/86\n",
    "```\n",
    "\n",
    "In this case, the ID of the \"20 Shakespear plays\" text set is `86`.\n",
    "\n",
    "We will also pass serveral word frequecies options to the Word Frequencies API. These options are:\n",
    "\n",
    "- `blockMethod`: We set the block method to `0`(By text), which makes each text from the text set as a single segment.\n",
    "- `numberOfNGrams`: We want to find the frequencies of `6` adjacent words.\n",
    "- `outputSize`: We set it to `10` as we are only interested in the top 10 most frequent words.\n",
    "- `excludeWords`: We are excluding some common punctuation marks from our analysis.\n",
    "\n",
    "To view more details about options of Word Frequencies API, read the <ins>API documentation</ins>.\n",
    "\n",
    "_@Hugh: more explainations about these options may be specified here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data = {\n",
    "    'textSet': 86,\n",
    "    'option':{\n",
    "        'blockMethod': 0,       # Segment by text\n",
    "        'numberOfNGrams' : 6,\n",
    "        'outputSize': 10,\n",
    "        'excludeWords': [\"[\",\"\\\\\", \"]\", \"_\", \"`\", \"!\", \"\\\"\", \"#\", \"%\", \"'\", \"(\", \")\", \"+\", \",\", \"-\", \"â€“\", \".\", \"/\", \":\", \";\", \"{\", \"|\", \"}\", \"=\", \"~\", \"?\" ],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SIA API accept JSON as the request data. Here we have constructed a Python dictionary object with the text set identifier and the word frequencies options. Next we are going to put all things together and use the [Requests](https://requests.readthedocs.io/en/latest/) module to send the request to the SIA API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the API Request\n",
    "response = requests.post(request_url, json=request_data, headers={\"X-API-KEY\": api_key}, timeout=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have specified the API endpoint, request data we defined earlier and the `X-API-KEY` HTTP header for the API request and received the response. Please note that the API call can take serveral minutes to finish based on the size of the text or text set. Therefore, we have set the request timeout to `1200` seconds.\n",
    "\n",
    "Before we start unpacking the response data, we want to make sure the API call was successful by checking the HTTP response code. Read the <ins>API documentation</ins> for all error codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{response.status_code} {response.reason}\")\n",
    "assert response.status_code == 200\n",
    "response_data = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the response data ready from Word Frequencies API. We firstly want to display the top most frequent 6-grams in a table that we can know what they are and the frequencies in each text from the text sets. We are going to firstly unpack the response data and make it into a tabular data format. Note that the words returned from the Word Frequencies API are sorted from the highest to lowest frequency by default, and the orders of words returned from blocks are consistent. Read more about the <ins>response data</ins> of Word Frequencies API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_headers = []\n",
    "table_rows = []\n",
    "for block in response_data['blocks']:\n",
    "    # Add table header row.\n",
    "    if len(table_headers) == 0:\n",
    "        table_headers.append(['Word'])\n",
    "    table_headers[0].append(block['name'])\n",
    "\n",
    "    # Add data rows.\n",
    "    for i in range(len(block['frequencies'])):\n",
    "        frequency = block['frequencies'][i]\n",
    "        # Check whether the row has been created. If not, initialise the row with the word text.\n",
    "        if i > len(table_rows) - 1:\n",
    "            table_rows.append([frequency['word']])\n",
    "        # Append the word frequency to its corresponding row.\n",
    "        table_rows[i].append(frequency['value'])\n",
    "\n",
    "    # Add the \"Word Types\" row.\n",
    "    i += 1\n",
    "    if i > len(table_rows) - 1:\n",
    "        table_rows.append(['Word Types'])\n",
    "    table_rows[i].append(block['uniqueWordCount'])\n",
    "\n",
    "    # Add the \"Size\" row.\n",
    "    i += 1\n",
    "    if i > len(table_rows) - 1:\n",
    "        table_rows.append(['Size'])\n",
    "    table_rows[i].append(block['size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created two 2-dimension list. The `table_headers` list contains a single table header row of the block names. The `table_rows` list contains rows of frequencies of the 6-grams. The next step is to generate the HTML markups based on the tabular data and render it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the opening tags of container and table elements.\n",
    "html = '<div style=\"overflow-x: auto; margin-top: 40px;\"><table border=\"1\">'\n",
    "\n",
    "# Append table headers.\n",
    "for table_row in table_headers:\n",
    "    html += '<tr>'\n",
    "    for table_cell in table_row:\n",
    "        html += f'<th style=\"white-space: nowrap;\">{table_cell}</td>'\n",
    "    html += '</tr>'\n",
    "\n",
    "# Append HTML for table rows.\n",
    "for table_row in table_rows:\n",
    "    html += '<tr>'\n",
    "    for table_cell in table_row:\n",
    "        html += f'<td>{table_cell}</td>'\n",
    "    html += '</tr>'\n",
    "\n",
    "# Close the table and container elements.\n",
    "html += '</table></div>'\n",
    "\n",
    "# Render the HTML.\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our table rendered. From the table, we can see that two 6-grams each appear 5 times in a single play; one 6-gram appears in 4 different plays. The 6-grams can be overlapping (\"hey ho the wind and the\", \"ho the wind and the rain\").\n",
    "\n",
    "_@Hugh: more insights about the table can be described here_\n",
    "\n",
    "Next, we are going to create a bar chart to show the overall top 10 most frequent 6-grams with their frequencies in the text set. Because word frequencies returned from the API are in the context of each block, we will need to sum up the word frequencies from all blocks first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to hold word frequencies\n",
    "word_frequency_map = {}\n",
    "for block in response_data[\"blocks\"]:\n",
    "    for freq in block['frequencies']:\n",
    "        word = freq['word']\n",
    "        value = freq['value']\n",
    "        word_frequency_map[word] = word_frequency_map.get(word, 0) + value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a dictionary with the words as keys, and total word frequencies in the text set as values. Then we will use the [Matplotlib](https://matplotlib.org/) library to visualise this. We set the 6-grams as the x-axis, and their word frequencies as the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(list(word_frequency_map.keys()), list(word_frequency_map.values()), color='blue')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 6-grams in the 20 plays by frequency ')\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_@Hugh: some insights about the chart_"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
